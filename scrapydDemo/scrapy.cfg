# Automatically created by: scrapy startproject
#
# For more information about the [deploy] section see:
# https://scrapyd.readthedocs.org/en/latest/deploy.html
[settings]
default = scrapydDemo.settings
[deploy]
url = http://localhost:6800/
project = scrapydDemo
username=dmoz
password=123456

#1.首先:新打开一个终端运行 scrapyd,把scrapy 的server端口打开
#1.first of all :open a terminal to run : 'scrapyd' ,which start the scrapy's service
#2.运行命令进行部署: scrapyd-deploy default -p scrapydDemo
#2.then, open another terminal to run: 'scrapyd-deploy default -p scrapydDemo' , whitch deploy the project on service  
#例如下面
#$ scrapyd-deploy default -p scrapydDemo(其实后面不用加,直接 scrapyd-deploy 就可以了)
#部署成功 返回如下信息/once the deploy is success,we get some infos as follows
#$ Packing version 1443188926(这个版本号是默认的当前时间戳)
#$ Deploying to project "scrapydDemo" in http://localhost:6800/addversion.json
#$ Server response (200):
#$ {"status": "ok", "project": "scrapydDemo", "version": "1443188926", "spiders": 1, "node_name": "nancheng-Lenovo-G480"}
#3.运行spider:  
#3.run spider on server
# 在根目录下面编写了一个url请求,这个请求就是下面这样,实际上,还是相当于发出了一个curl请求
#create a py file anywhere(of course,better in the project's root direction ),we code some thing to mock a nimal get request  like this (my named dmoz-start.py),then run it as py
#coding=utf-8         
# dmoz_data={'project':'scrapydDemo','spider':'dmoz'}
# dmoz_data_urlencode=urllib.urlencode(dmoz_data)
# requesurl="http://localhost:6800/schedule.json"
# req=urllib2.Request(url=requesurl,data=dmoz_data_urlencode)
# res_data=urllib2.urlopen(req)





